{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNg9LIczURV9gZbdeba447Q",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/boshuaiYu/Deeplearning_pytorch/blob/main/Chapter_2.1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "第二章 数据操作"
      ],
      "metadata": {
        "id": "h6fpF0AkRWfv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TmNy-4jZQ68d",
        "outputId": "cb6939bd-786e-4637-d446-04a16ce91890"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.12.1+cu113\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "print(torch.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.arange(12)\n",
        "x, x.shape, x.size(), x.numel()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rhP1Ikm7RGox",
        "outputId": "57fe56bf-2c56-42b4-d054-ab42e7cbc23b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11]),\n",
              " torch.Size([12]),\n",
              " torch.Size([12]),\n",
              " 12)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. 在numpy中， shape得到了数组的形状，size统计了数组总共的大小，而size也可以在某个轴上进行统计，这样也就达到了得到数组某个轴上形状的目的。\n",
        "2. 在torch模块，tensor.size()和tensor.shape都会返回Tensor的形状大小，注意size带()\n",
        "3. torch中，得到张量元素总数的方法是x.numel()\n"
      ],
      "metadata": {
        "id": "F7DU8DWeTqWG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = x.reshape(3,4) # reshape只改变张量的形状，并不改变其元素值\n",
        "X = x.reshape(-1,4)\n",
        "X = x.reshape(3,-1)\n",
        "# 以上三种都是一样的"
      ],
      "metadata": {
        "id": "bMB3CVI9SQKY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 创建全0，全1，随机分布的初始化张量\n",
        "torch.zeros((2,3,4)) , torch.ones((2,3,4)), torch.randn(2,3,4)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UV1D7MHiUr8f",
        "outputId": "eec38dda-0dc1-40c9-9f64-33096b030857"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[[0., 0., 0., 0.],\n",
              "          [0., 0., 0., 0.],\n",
              "          [0., 0., 0., 0.]],\n",
              " \n",
              "         [[0., 0., 0., 0.],\n",
              "          [0., 0., 0., 0.],\n",
              "          [0., 0., 0., 0.]]]), tensor([[[1., 1., 1., 1.],\n",
              "          [1., 1., 1., 1.],\n",
              "          [1., 1., 1., 1.]],\n",
              " \n",
              "         [[1., 1., 1., 1.],\n",
              "          [1., 1., 1., 1.],\n",
              "          [1., 1., 1., 1.]]]), tensor([[[-0.0967,  1.2059, -1.3103,  1.3358],\n",
              "          [-0.5490, -1.0875, -0.5596, -0.3418],\n",
              "          [-0.6110, -1.1053,  0.5342,  0.1262]],\n",
              " \n",
              "         [[ 0.4062,  0.1698, -1.2688,  0.0349],\n",
              "          [-1.0562, -1.2842,  0.6903,  1.0508],\n",
              "          [ 0.9717,  0.0163,  0.2937, -2.6779]]]))"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "可以通过提供包含数值的Python列表（或嵌套列表），来为所需张量中的每个元素赋予确定值。 在这里，最外层的列表对应于轴0，内层的列表对应于轴1"
      ],
      "metadata": {
        "id": "pRdrOgGWWRt1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "A = torch.tensor([[2, 1, 4, 3], [1, 2, 3, 4], [4, 3, 2, 1]])\n",
        "A.shape, A.size(0), A.size(1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iPr3dpVAVo13",
        "outputId": "f85eb543-f6bb-4a55-ea49-a780eac4a743"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([3, 4]), 3, 4)"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "A = torch.tensor([[[2, 1, 4, 3], [1, 2, 3, 4], [4, 3, 2, 1]]])\n",
        "A.shape, A.size(0), A.size(1), A.size(2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Hv63cI9WacH",
        "outputId": "418b5005-190b-41a9-fcc2-456bae3cf3a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([1, 3, 4]), 1, 3, 4)"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "把多个张量连结（concatenate）在一起， 把它们端对端地叠起来形成一个更大的张量。 我们只需要提供张量列表，并给出沿哪个轴连结。下面分别演示了当我们沿行（轴-0，形状的第一个元素） 和按列（轴-1，形状的第二个元素）"
      ],
      "metadata": {
        "id": "fBNDrlMia7aj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = torch.arange(12,dtype=torch.float32).reshape(3,4)  \n",
        "Y = torch.tensor([[2.0, 1, 4, 3], [1, 2, 3, 4], [4, 3, 2, 1]])  \n",
        "A = torch.cat((X,Y), dim=0)\n",
        "B = torch.cat((X,Y), dim=1) \n",
        "X.shape, Y.shape, A, B, A.shape, B.shape                                                                                                                         "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DjcH_MTLW48m",
        "outputId": "a81b2cca-08bc-436f-fae5-4d5a03b6b300"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([3, 4]), torch.Size([3, 4]), tensor([[ 0.,  1.,  2.,  3.],\n",
              "         [ 4.,  5.,  6.,  7.],\n",
              "         [ 8.,  9., 10., 11.],\n",
              "         [ 2.,  1.,  4.,  3.],\n",
              "         [ 1.,  2.,  3.,  4.],\n",
              "         [ 4.,  3.,  2.,  1.]]), tensor([[ 0.,  1.,  2.,  3.,  2.,  1.,  4.,  3.],\n",
              "         [ 4.,  5.,  6.,  7.,  1.,  2.,  3.,  4.],\n",
              "         [ 8.,  9., 10., 11.,  4.,  3.,  2.,  1.]]), torch.Size([6, 4]), torch.Size([3, 8]))"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "广播机制：形状不同，我们仍然可以通过调用 广播机制（broadcasting mechanism）来执行按元素操作。 这种机制的工作方式如下：首先，通过适当复制元素来扩展一个或两个数组， 以便在转换之后，两个张量具有相同的形状。 其次，对生成的数组执行按元素操作"
      ],
      "metadata": {
        "id": "J6Wb6Ye1cwcj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "a = torch.arange(3).reshape((3, 1))\n",
        "b = torch.arange(2).reshape((1, 2))\n",
        "a, b, a+b"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qn3t0aXJbz29",
        "outputId": "7eee6303-a55b-4446-8a12-4c87d602385d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[0],\n",
              "         [1],\n",
              "         [2]]), tensor([[0, 1]]), tensor([[0, 1],\n",
              "         [1, 2],\n",
              "         [2, 3]]))"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. 运行一些操作可能会导致为新结果分配内存。 例如，如果我们用Y = X + Y，我们将取消引用Y指向的张量，而是指向新分配的内存处的张量,增加了没必要的内存\n",
        "2. 可以使用切片表示法将操作的结果分配给先前分配的数组，例如Y[:] = <expression>。 为了说明这一点，我们首先创建一个新的矩阵Z，其形状与另一个Y相同， 使用zeros_like来分配一个全的块\n",
        "3. 如果在后续计算中没有重复使用X， 我们也可以使用X[:] = X + Y或X += Y来减少操作的内存开销\n"
      ],
      "metadata": {
        "id": "roORbf8veBLq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = torch.arange(3).reshape((3, 1))\n",
        "Y = torch.arange(3).reshape((3, 1))\n",
        "Z = torch.zeros_like(Y)\n",
        "print('id(Z):', id(Z))\n",
        "Z[:] = X + Y\n",
        "print('id(Z):', id(Z))\n",
        "before = id(X)\n",
        "X += Y\n",
        "id(X) == before\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Zw89OFMdyTM",
        "outputId": "960b3c99-9ee1-4536-c9c4-d7cd41c88458"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "id(Z): 140512118462416\n",
            "id(Z): 140512118462416\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "深度学习框架定义的张量转换为NumPy张量（ndarray）很容易，反之也同样容易。 torch张量和numpy数组将共享它们的底层内存，就地操作更改一个张量也会同时更改另一个张量"
      ],
      "metadata": {
        "id": "LpDCuEwtf-Iq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = torch.tensor([1.0, 2, 4, 8])\n",
        "print(id(X))\n",
        "A = X.numpy()\n",
        "print(id(A))\n",
        "X.add_(1), A"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1i6flurKej4j",
        "outputId": "6d86f59f-2db0-49ba-803b-134411256a5e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "140512118146256\n",
            "140512118147184\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([2., 3., 5., 9.]), array([2., 3., 5., 9.], dtype=float32))"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "把numpy转成tensor a = numpy.ones(5)，\n",
        "1. b = torch.tensor(a) 相当于创建一个新tensor，不会共享内存\n",
        "2. b = torch.from_numpy(a) 相当于从获取numpy中的值，会共享内存\n",
        "\n",
        "所以numpy和from_numpy会共享内存，torch.tensor不会共享内存"
      ],
      "metadata": {
        "id": "pL9CO70xgK36"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "a = np.ones(5)\n",
        "print(id(a))\n",
        "b1 = torch.tensor(a) # 不会共享内存\n",
        "b2 = torch.from_numpy(a) # 会共享内存\n",
        "print(id(b1))\n",
        "print(id(b2))\n",
        "a += 1\n",
        "b1, b2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7z4gHKVPgIfT",
        "outputId": "ec15864f-af77-48ae-d696-90486a40dc8d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "140512117830160\n",
            "140512118206000\n",
            "140512118203600\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([1., 1., 1., 1., 1.], dtype=torch.float64),\n",
              " tensor([2., 2., 2., 2., 2.], dtype=torch.float64))"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    }
  ]
}